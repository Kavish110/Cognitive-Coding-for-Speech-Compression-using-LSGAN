# Quick Start Guide - Cognitive Speech Compression

Get up and running in 5 minutes!

## 1. Install Dependencies (2 min)

```bash
# Navigate to project
cd cognitive_speech_compression

# Install packages
pip install -r requirements.txt
```

Verify installation:
```bash
python -c "import torch; import torchaudio; print('âœ“ Ready!')"
```

## 2. Download & Prepare Dataset (15 min)

**Option A: Quick Test (No real data)**
```bash
# Just run training on dummy data - useful to test code
python scripts/train.py
```

**Option B: Use Real LibriSpeech Data**

1. Download (do this once, ~6GB):
   ```bash
   # Visit: https://www.openslr.org/12
   # Download: train-clean-100 (6.3GB)
   # Extract to: data/LibriSpeech/trainclean100/
   ```

2. Prepare manifests:
   ```bash
   python scripts/prep_librispeech.py \
     --root data/LibriSpeech/trainclean100 \
     --out data/librispeech_100h/train.json
   ```

## 3. Train (30+ min depending on config)

```bash
# Using default config
python scripts/train.py

# Custom config
python scripts/train.py --cfg configs/default.yaml
```

You'll see output like:
```
step 100/100 | D 0.2341 | G 1.2543 (adv 0.1234 fm 0.3456 mel 0.5678 cc_s 0.0234 cc_l 0.0341)
step 200/100 | D 0.1856 | G 1.1234 (adv 0.0987 fm 0.3123 mel 0.5234 cc_s 0.0156 cc_l 0.0234)
...
```

### What do those numbers mean?

- **D**: Discriminator loss (lower = better at detecting fake audio) â†’ goal: 0.1-0.5
- **G**: Generator/reconstruction loss (lower = better reconstruction) â†’ goal: decreasing
- **mel**: Most important! Spectral similarity (lower = better audio quality) â†’ goal: < 0.1
- **cc_s/cc_l**: Content preservation losses (lower = better) â†’ goal: < 0.05

## 4. Check Results

After training finishes, you'll find:

### Training Metrics
```
plots/training_metrics.png
```
Shows 6 plots:
- Discriminator loss trend
- Generator loss components
- Loss balance (D vs G)
- Compression quality (mel loss)

### Feature Visualization
```
plots/cc_features.png
```
Shows what the encoder learned:
- Top: Short-context features (10ms resolution)
- Bottom: Long-context features (40ms resolution)
- Bright colors = high activation

### Audio Reconstruction
```
plots/waveform_comparison.png
```
Shows original vs reconstructed:
- Top: Original audio
- Middle: Reconstructed audio
- Bottom: Error signal (should be small!)

### Saved Models
```
checkpoints/codec_step2000.pt
checkpoints/codec_step4000.pt
...
```
Use these for inference!

## 5. Inference (Reconstruct Audio)

```bash
python scripts/infer.py \
  --wav input.wav \
  --ckpt checkpoints/codec_step100.pt \
  --cfg configs/default.yaml \
  --out reconstructed.wav
```

## Customization - 3 Things to Try

### âš¡ Train Faster (but lower quality)
Edit `configs/default.yaml`:
```yaml
train:
  total_steps: 10          # From 100 (much faster)
  batch_size: 2            # From 4 (less GPU memory)
```

### ðŸŽµ Better Audio Quality
```yaml
train:
  total_steps: 1000        # Much longer training
  batch_size: 8            # Larger batches

loss:
  lambda:
    mel: 100.0             # Focus on quality (from 50)
```

### ðŸš€ Use GPU (if available)
```bash
# Check if GPU available
python -c "import torch; print(torch.cuda.is_available())"

# Already auto-detects! Code runs on GPU if available
```

## File Structure for Reference

```
cognitive_speech_compression/
â”œâ”€â”€ README.md                    â† Full documentation
â”œâ”€â”€ QUICKSTART.md               â† This file
â”œâ”€â”€ OUTPUT_DESCRIPTION.md       â† What plots mean
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ default.yaml            â† Training settings (edit this!)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train.py               â† Training (main script)
â”‚   â”œâ”€â”€ infer.py               â† Audio reconstruction
â”‚   â””â”€â”€ prep_librispeech.py    â† Prepare dataset
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/                â† Neural network components
â”‚   â”œâ”€â”€ losses.py              â† Loss functions
â”‚   â”œâ”€â”€ datasets.py            â† Data loading
â”‚   â””â”€â”€ utils.py               â† Utilities
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ LibriSpeech/           â† Download data here
â”‚   â””â”€â”€ librispeech_100h/      â† Manifests (auto-generated)
â”œâ”€â”€ checkpoints/               â† Saved models (auto-generated)
â””â”€â”€ plots/                     â† Visualizations (auto-generated)
```

## Typical Training Output

```
Epoch 1: Training encoder and decoder
step 100/100 | D 0.5432 | G 3.2341 ...  (early, losses high)
step 200/100 | D 0.3211 | G 2.1234 ...  (stabilizing)
step 300/100 | D 0.2341 | G 1.2543 ...  (converging)
...
Generating final training plots...
âœ“ Saved metrics plot to plots/training_metrics.png
âœ“ Saved CC features plot to plots/cc_features.png
âœ“ Saved waveform comparison to plots/waveform_comparison.png
Training completed!
```

## Common Issues

| Issue | Fix |
|-------|-----|
| `ModuleNotFoundError: matplotlib` | `pip install matplotlib` |
| `CUDA out of memory` | Reduce `batch_size` in config |
| `No FLAC files found` | Download LibriSpeech first |
| `Plots not showing` | Install matplotlib: `pip install matplotlib` |
| `Audio quality poor` | Increase `total_steps` and mel loss weight |

## Next Steps

1. âœ… Train for longer: Change `total_steps: 1000` (or more!)
2. âœ… Improve quality: Increase `lambda['mel']` in config
3. âœ… Experiment: Try different architectures in `configs/`
4. âœ… Deploy: Use best checkpoint for inference
5. âœ… Analyze: Read OUTPUT_DESCRIPTION.md for detailed metrics explanation

## Need Help?

- **Full guide**: See `README.md`
- **Understanding outputs**: See `OUTPUT_DESCRIPTION.md`
- **Code issues**: Check `issues.txt`
- **Questions**: Review code comments in `src/models/`

---

**That's it!** You're ready to compress speech! ðŸŽ‰

